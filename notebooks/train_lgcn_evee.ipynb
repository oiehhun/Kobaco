{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from util import load_jsonl, load_node_csv\n",
    "\n",
    "from model.continuous_prompt import ContinuousPromptingLLM\n",
    "from model.rec_encoder import RecommendationContinuousPromptModel\n",
    "from model.projection import BasicProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171a60d88ef24c258de0d9011853f363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path='/home/bonbak/kobaco/data/kobaco.csv'\n",
    "device='cuda:3'\n",
    "save_dir = \"/SSL_NAS/bonbak/model/models--yanolja--EEVE-Korean-Instruct-2.8B-v1.0/snapshots/482db2d0ba911253d09342c34d0e42ac871bfea3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = load_jsonl(data_path)\n",
    "        self.input_text_list = self.set_input_text_list()\n",
    "        self.answer_list = self.set_answer_list()\n",
    "        self.continuous_prompt_input_list = self.set_continuous_prompt_input_list()\n",
    "\n",
    "    def set_input_text_list(self):\n",
    "        input_text_list = []\n",
    "        for data in self.data:\n",
    "            user = data['user_id']\n",
    "            prompt =f'사용자 {user}의 TV 프로그램 시청 기록:\\n'\n",
    "            for idx, item in enumerate(data['iteracted_items']):\n",
    "                prompt += f'{idx}. {item}\\n'\n",
    "            prompt +='\\n타겟 TV 프로그램:\\n* ' + data['target_item'] + '\\n\\n'\n",
    "            prompt += data['question']\n",
    "            input_text_list.append(prompt)\n",
    "        return input_text_list\n",
    "\n",
    "    def set_answer_list(self):\n",
    "        return [x['answer'] for x in self.data]\n",
    "    \n",
    "\n",
    "    def set_continuous_prompt_input_list(self):\n",
    "        # return [{'input_text_list':'\\n'.join([x['node_information'],x['edge_information']])} for x in self.data]\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_text_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_text_list[idx], self.answer_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = load_node_csv('../data/kobaco.csv', index_col='user_id')\n",
    "item_mapping = load_node_csv('../data/kobaco.csv', index_col='item_id')\n",
    "\n",
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, data_path, user_mapping, item_mapping):\n",
    "        self.user_mapping = user_mapping\n",
    "        self.item_mapping = item_mapping\n",
    "        super().__init__(data_path)\n",
    "        \n",
    "\n",
    "    def set_continuous_prompt_input_list(self):\n",
    "        continuous_prompt_input_list = []\n",
    "        for x in self.data:\n",
    "            interacted_items = list(map(lambda item:self.item_mapping[item], x['iteracted_items']))\n",
    "            target_item = [self.item_mapping[x['target_item']]]\n",
    "            item_ids = torch.Tensor(interacted_items+target_item).type(torch.long)\n",
    "            user_id = torch.Tensor([self.user_mapping[x['user_id']]]).type(torch.long)\n",
    "            continuous_prompt_input_list.append({'user_id':user_id, 'item_ids':item_ids})\n",
    "        return continuous_prompt_input_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_text_list[idx], self.continuous_prompt_input_list[idx], self.answer_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = '../output'\n",
    "MODEL_NAME = 'light-gcn'\n",
    "\n",
    "train_dataset = RecommendationDataset('../data/train.jsonl', user_mapping, item_mapping)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# test_dataset = RecommendationDataset('../data/test.jsonl', user_mapping, item_mapping)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "num_users, num_items = len(user_mapping), len(item_mapping)\n",
    "\n",
    "continuous_prompt_model = RecommendationContinuousPromptModel(num_users,num_items,'../data/train_edge_index.pt')\n",
    "projection_module = BasicProjection(continuous_prompt_model.model.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContinuousPromptingLLM(\n",
    "    \"/SSL_NAS/bonbak/model/models--yanolja--EEVE-Korean-Instruct-2.8B-v1.0/snapshots/482db2d0ba911253d09342c34d0e42ac871bfea3\",\n",
    "    continuous_prompt_model, \n",
    "    continuous_prompt_model.model.embedding_dim\n",
    ")\n",
    "\n",
    "model.continuous_prompt_model.model.load_state_dict(torch.load(f'{SAVE_DIR}/model/{MODEL_NAME}.bin'))\n",
    "\n",
    "continuous_prompt_model.to(device)\n",
    "model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.projection_module.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.continuous_prompt_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(data, filename='plot.png'):\n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.title('Train Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(filename)\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "c = 0\n",
    "loss_log_list = []\n",
    "min_loss = 1000000\n",
    "accumulate_step = 8\n",
    "\n",
    "def mean(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for input_text, continuous_prompt_input, answer_list in train_dataloader:\n",
    "        inputs_embeds, attention_mask, labels = model.make_seq2seq_input_label(input_text,continuous_prompt_input,answer_list, embedding_first=True)\n",
    "\n",
    "        generated_output = model.llm_model.forward(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask = attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "        generated_output.loss.backward()\n",
    "        \n",
    "        if c % accumulate_step == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        loss_log_list.append(generated_output.loss.item())\n",
    "        \n",
    "        if c % 80 == 0 and c!=0:\n",
    "            cur_loss = mean(loss_log_list[-accumulate_step:])\n",
    "            if min_loss > cur_loss:\n",
    "                model.eval()\n",
    "                model.to('cpu')\n",
    "                min_loss = cur_loss\n",
    "                torch.save(model.projection_module.state_dict(), f'{SAVE_DIR}/model/{MODEL_NAME}-projection.bin')\n",
    "                torch.save(model.continuous_prompt_model.state_dict(), f'{SAVE_DIR}/model/{MODEL_NAME}-encoder.bin')\n",
    "\n",
    "                inputs_embeds, attention_mask = model.make_input_embed(input_text,continuous_prompt_input, embedding_first=True)\n",
    "                output = model.llm_model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1)\n",
    "                print(model.llm_tokenizer.decode(output[0]))\n",
    "                plot_and_save(loss_log_list, f'{SAVE_DIR}/loss/{MODEL_NAME}.png')\n",
    "\n",
    "                model.train()\n",
    "                model.to(device)\n",
    "\n",
    "            print(f'step {c} | cur_loss : {cur_loss:.4f} | min_loss : {min_loss:.4f} ')\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5666 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_text, continuous_prompt_input, answer_list \u001b[38;5;129;01min\u001b[39;00m tqdm(test_dataset):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m         inputs_embeds, attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_input_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous_prompt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mllm_model\u001b[38;5;241m.\u001b[39mgenerate(inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m         pred\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mllm_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/kobaco/notebooks/../model/continuous_prompt.py:33\u001b[0m, in \u001b[0;36mContinuousPromptingLLM.make_input_embed\u001b[0;34m(self, text_input, continuous_prompt_input_dict, padding_side, embedding_first)\u001b[0m\n\u001b[1;32m     30\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_tokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(text_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m word_embedding_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()(torch\u001b[38;5;241m.\u001b[39mtensor(x)\u001b[38;5;241m.\u001b[39mto(device)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tokenized_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 33\u001b[0m continuous_prompt_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuous_prompt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontinuous_prompt_input_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continuous_prompt_vector\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     35\u001b[0m     continuous_prompt_vector \u001b[38;5;241m=\u001b[39m continuous_prompt_vector\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/GraphToken/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GraphToken/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kobaco/notebooks/../model/rec_encoder.py:19\u001b[0m, in \u001b[0;36mRecommendationContinuousPromptModel.forward\u001b[0;34m(self, user_id, item_ids)\u001b[0m\n\u001b[1;32m     16\u001b[0m item_ids \u001b[38;5;241m=\u001b[39m item_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m users_emb_final, _, items_emb_final, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(edge_index)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43musers_emb_final\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems_emb_final\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = []\n",
    "label = []\n",
    "\n",
    "test_dataset = RecommendationDataset('../data/test.jsonl', user_mapping, item_mapping)\n",
    "for input_text, continuous_prompt_input, answer_list in tqdm(test_dataset):\n",
    "    with torch.no_grad():\n",
    "        inputs_embeds, attention_mask = model.make_input_embed([input_text], continuous_prompt_input, embedding_first=True)\n",
    "        output = model.llm_model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1)\n",
    "        pred.append(model.llm_tokenizer.batch_decode(output, skip_special_tokens=True)[0])\n",
    "        label.append(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgcn = model.continuous_prompt_model.model.to(device)\n",
    "users_emb_final, _, items_emb_final, _ = lgcn(model.continuous_prompt_model.edge_index.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([16729, 64]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_emb_final[torch.tensor([1])].size(), items_emb_final.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def convert_answer(answer):\n",
    "    converted = []\n",
    "    for a in answer:\n",
    "        a = a.strip()\n",
    "        if a == '예':\n",
    "            converted.append(1)\n",
    "        elif a == '아니':\n",
    "            converted.append(0)\n",
    "        else:\n",
    "            converted.append(-1)\n",
    "    return np.array(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = convert_answer(pred)\n",
    "y_true = convert_answer(test_dataset.answer_list)\n",
    "new_y_pred = y_pred[np.where(y_pred!=-1)]\n",
    "new_y_true = y_true[np.where(y_pred!=-1)]\n",
    "missed = np.where(y_pred==-1)\n",
    "print('miss rate:',len(missed[0]) / len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy = accuracy_score(new_y_true, new_y_pred)\n",
    "f1 = f1_score(new_y_true, new_y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphToken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
